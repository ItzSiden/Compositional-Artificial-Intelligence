# Compositional-Artificial-Intelligence
## ğŸ§  MSCP â€” Multi-System Compositional Pipeline

A **5-Layer local AI system** that runs entirely on CPU with no GPU required.
It beats cloud AI for private, domain-specific knowledge tasks by combining:

- **Layer 1** â€” Llama 3.2 1B (quantized, runs in <1GB RAM)
- **Layer 2** â€” FAISS vector database + MiniLM embeddings
- **Layer 3** â€” NetworkX concept graph with weighted edges
- **Layer 4** â€” Short-term rolling memory buffer (deque)
- **Layer 5** â€” Intelligent prompt assembler + persona injector

---

## ğŸ—‚ï¸ Project Structure

```
mscp/
â”œâ”€â”€ main.py                  â† The main chat loop (run this)
â”œâ”€â”€ layer1_engine.py         â† LLM inference (llama.cpp)
â”œâ”€â”€ layer2_vector.py         â† FAISS vector brain
â”œâ”€â”€ layer3_graph.py          â† NetworkX concept graph
â”œâ”€â”€ layer4_memory.py         â† Short-term memory buffer
â”œâ”€â”€ layer5_assembler.py      â† Prompt assembler
â”œâ”€â”€ setup.sh                 â† One-shot setup script
â”œâ”€â”€ requirements.txt         â† Python dependencies
â”œâ”€â”€ knowledge_base/          â† Drop .txt files here to teach the AI
â”‚   â””â”€â”€ python_reference.txt â† Sample knowledge (Python & Linux)
â”œâ”€â”€ graph_data/              â† Auto-generated concept graph storage
â”œâ”€â”€ models/                  â† Put your .gguf model file here
â”œâ”€â”€ brain.index              â† Auto-generated FAISS index
â””â”€â”€ brain_meta.pkl           â† Auto-generated FAISS metadata
```

---

## ğŸš€ Setup on GitHub Codespaces

### Step 1: Open in Codespaces
Click **Code â†’ Codespaces â†’ Create codespace** on this repo.

### Step 2: Run the setup script
```bash
chmod +x setup.sh
./setup.sh
```
This will:
- Create a Python virtual environment (`mscp_env/`)
- Install all dependencies
- Download the Llama 3.2 1B model (~700MB) from HuggingFace

### Step 3: Activate the environment
```bash
source mscp_env/bin/activate
```

### Step 4: Run MSCP
```bash
python main.py
```

---

## ğŸ–ï¸ Manual Model Download (if setup.sh fails)

1. Go to: https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF
2. Download: `Llama-3.2-1B-Instruct-Q4_K_M.gguf`
3. Place it in the `models/` folder

Or via terminal:
```bash
pip install huggingface_hub
python -c "
from huggingface_hub import hf_hub_download
hf_hub_download(
    repo_id='bartowski/Llama-3.2-1B-Instruct-GGUF',
    filename='Llama-3.2-1B-Instruct-Q4_K_M.gguf',
    local_dir='models'
)
"
```

---

## ğŸ’¬ Chat Commands

| Command    | Description                                          |
|------------|------------------------------------------------------|
| `/graph`   | Visualize the concept graph â†’ saves `concept_graph.png` |
| `/memory`  | Show the current short-term chat buffer              |
| `/ingest`  | Re-scan `knowledge_base/` for new `.txt` files       |
| `/clear`   | Clear the short-term memory buffer                   |
| `/help`    | Show all commands                                    |
| `/exit`    | Quit MSCP                                            |

---

## ğŸ“š Teaching the AI New Knowledge

Drop any `.txt` file into the `knowledge_base/` folder, then type `/ingest`:

```bash
# Example: Add your server docs
echo "Server rack A contains: web1 (192.168.1.10), web2 (192.168.1.11)" > knowledge_base/servers.txt

# In MSCP chat:
/ingest
```

The AI will instantly know this â€” no retraining needed.

---

## ğŸ—ï¸ How the 5 Layers Work Together

```
[USER PROMPT]
      â”‚
      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layer 4: Short-Term â”‚  â† Checks last 5 messages for context
â”‚ Memory Buffer       â”‚    Old messages â†’ compressed to FAISS
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
     â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”
     â–¼            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layer 3 â”‚  â”‚ Layer 2  â”‚
â”‚ Concept â”‚  â”‚ Vector   â”‚
â”‚ Graph   â”‚  â”‚ Brain    â”‚
â”‚NetworkX â”‚  â”‚ FAISS    â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
     â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layer 5: Assembler  â”‚  â† Builds the mega-prompt
â”‚ + Personality       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layer 1: LLM Core   â”‚  â† Llama 3.2 1B (CPU, <1GB)
â”‚ llama.cpp           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â–¼
    [AI RESPONSE]
```

---

## âš¡ System Requirements

| Resource | Minimum    | Recommended |
|----------|------------|-------------|
| RAM      | 2 GB       | 4 GB        |
| Disk     | 2 GB       | 4 GB        |
| CPU      | Any x86_64 | 4+ cores    |
| GPU      | Not needed | Not needed  |

GitHub Codespaces free tier (4-core, 8GB RAM) works perfectly.

---

## ğŸ”§ Customizing the Persona

Edit `layer5_assembler.py` and change `SYSTEM_PERSONA`:

```python
SYSTEM_PERSONA = """You are a senior DevOps engineer.
You specialize in Linux, Docker, and Kubernetes.
Always show complete commands with explanations."""
```

---

## ğŸ§© Adding a Bigger Model

For better quality, swap to a 3B model (needs ~3GB RAM):

1. Download `Llama-3.2-3B-Instruct-Q4_K_M.gguf` from HuggingFace
2. Place in `models/`
3. Edit `layer1_engine.py`:
```python
MODEL_FILE = "Llama-3.2-3B-Instruct-Q4_K_M.gguf"
```
